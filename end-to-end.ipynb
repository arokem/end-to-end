{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An end-to-end pipeline for the analysis of HCP diffusion MRI data using Dipy\n",
    "\n",
    "From the S3 bucket to tract profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as op\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data from AWS S3\n",
    "\n",
    "We assume that you have a file '.aws/credentials', that includes a section: \n",
    "\n",
    "    [hcp]\n",
    "    AWS_ACCESS_KEY_ID=XXXXXXXXXXXXXXXX\n",
    "    AWS_SECRET_ACCESS_KEY=XXXXXXXXXXXXXXXX\n",
    "with the credentials you got [from HCP](https://wiki.humanconnectome.org/display/PublicData/How+To+Connect+to+Connectome+Data+via+AWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = {'./bvals':'HCP/994273/T1w/Diffusion/bvals', \n",
    "              './bvecs':'HCP/994273/T1w/Diffusion/bvecs', \n",
    "              './data.nii.gz':'HCP/994273/T1w/Diffusion/data.nii.gz'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import botocore.session\n",
    "import boto3\n",
    "boto3.setup_default_session(profile_name='hcp')\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket('hcp-openaccess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in data_files.keys():\n",
    "    if not op.exists(k):\n",
    "        bucket.download_file(data_files[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dipy.core.gradients as dpg\n",
    "import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the gradient table, setting the threshold to 10, so that low values are considered as 0:\n",
    "gtab = dpg.gradient_table('./bvals', './bvecs', b0_threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = nib.load('./data.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = img.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brain extraction (median Otsu)\n",
    "\n",
    "See (example and explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We look at the average non diffusion-weighted as data for brain extraction\n",
    "mean_b0 = np.mean(data[..., gtab.b0s_mask], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/skimage/filter/__init__.py:6: skimage_deprecation: The `skimage.filter` module has been renamed to `skimage.filters`.  This placeholder module will be removed in v0.13.\n",
      "  warn(skimage_deprecation('The `skimage.filter` module has been renamed '\n"
     ]
    }
   ],
   "source": [
    "from dipy.segment.mask import median_otsu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not op.exists('./brain_mask.nii.gz'):\n",
    "    from dipy.segment.mask import median_otsu\n",
    "    _, brain_mask = median_otsu(mean_b0, 4, 2, False, vol_idx=np.where(gtab.b0s_mask), dilate=1)\n",
    "    nib.save(nib.Nifti1Image(brain_mask.astype(int), img.affine), 'brain_mask.nii.gz')\n",
    "else: \n",
    "    brain_mask = nib.load('brain_mask.nii.gz').get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising (NLMEANS)\n",
    "\n",
    "See [example and explanations](http://nipy.org/dipy/examples_built/denoise_nlmeans.html#example-denoise-nlmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not op.exists('./denoised_data.nii.gz'):\n",
    "    from dipy.denoise import nlmeans\n",
    "    from dipy.denoise.noise_estimate import estimate_sigma\n",
    "    sigma = estimate_sigma(data)\n",
    "    denoised_data = nlmeans.nlmeans(data, sigma=sigma, mask=brain_mask)\n",
    "    nib.save(nib.Nifti1Image(denoised_data, img.affine), 'denoised_data.nii.gz')\n",
    "else:\n",
    "    denoised_data = nib.load('denoised_data.nii.gz').get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (DTI)\n",
    "\n",
    "See [example and explanations](http://nipy.org/dipy/examples_built/reconst_dti.html#example-reconst-dti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not op.exists('./dti_fa.nii.gz'):\n",
    "    import dipy.reconst.dti as dti\n",
    "    ten_model = dti.TensorModel(gtab)\n",
    "    ten_fit = ten_model.fit(denoised_data, mask=brain_mask)\n",
    "    fa = ten_fit.fa \n",
    "    md = ten_fit.md\n",
    "    nib.save(nib.Nifti1Image(fa, img.affine), 'dti_fa.nii.gz')\n",
    "    nib.save(nib.Nifti1Image(md, img.affine), 'dti_md.nii.gz')\n",
    "else:\n",
    "    fa = nib.load('./dti_fa.nii.gz').get_data()\n",
    "    md = nib.load('./dti_md.nii.gz').get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation (FA)\n",
    "\n",
    "FA is a measure of the anisotropy of diffusion in each voxel in the image. Because white matter tend to have higher anisotropy, we can use it as a way to roughly segment the image into portions that contain white matter and the non-white matter portion of the image. This allows us to reduce the computational demand of subsequent steps by only performing them in the masked regions of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wm_mask = np.zeros(fa.shape, dtype=bool)\n",
    "wm_mask[fa>0.2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (CSA) and tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not op.exists('./csa_tracks.trk'):\n",
    "    from dipy.reconst.shm import CsaOdfModel\n",
    "    from dipy.data import default_sphere\n",
    "    from dipy.direction import peaks_from_model\n",
    "    from dipy.tracking.local import LocalTracking\n",
    "    from dipy.io.trackvis import save_trk\n",
    "\n",
    "    csa_model = CsaOdfModel(gtab, sh_order=6)\n",
    "    csa_peaks = peaks_from_model(csa_model, denoised_data, default_sphere,\n",
    "                                 relative_peak_threshold=.8,\n",
    "                                 min_separation_angle=45,\n",
    "                                 mask=wm_mask)\n",
    "\n",
    "    from dipy.tracking.local import ThresholdTissueClassifier\n",
    "\n",
    "    classifier = ThresholdTissueClassifier(csa_peaks.gfa, .1)\n",
    "\n",
    "    from dipy.tracking import utils\n",
    "    seeds = utils.seeds_from_mask(wm_mask, density=[2, 2, 2], affine=img.affine)\n",
    "\n",
    "    # Initialization of LocalTracking. The computation happens in the next step.\n",
    "    streamlines = LocalTracking(csa_peaks, classifier, seeds, img.affine, step_size=.5)\n",
    "\n",
    "    # Compute streamlines and store as a list.\n",
    "    streamlines = [s for s in streamlines if s.shape[0]>10]\n",
    "\n",
    "    save_trk(\"csa_tracks.trk\", streamlines, img.affine, fa.shape)\n",
    "else:\n",
    "    tv = nib.trackvis.read(\"csa_tracks.trk\")\n",
    "    streamlines = [t[0] for t in tv[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Registration to a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dipy.data import read_mni_template\n",
    "from dipy.align.metrics import CCMetric\n",
    "from dipy.align.imwarp import SymmetricDiffeomorphicRegistration\n",
    "\n",
    "MNI_T2 = read_mni_template()\n",
    "MNI_T2_data = MNI_T2.get_data()\n",
    "MNI_T2_affine = MNI_T2.get_affine()\n",
    "use_metric = CCMetric(3, sigma_diff=2.0)\n",
    "sdr = SymmetricDiffeomorphicRegistration(use_metric, [10, 10, 5],\n",
    "                                         step_length=0.25)\n",
    "mapping = sdr.optimize(MNI_T2_data, mean_b0,\n",
    "                       static_grid2world=img.affine,\n",
    "                       moving_grid2world=MNI_T2_affine,\n",
    "                       prealign=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e175f815a2f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLOCC_ni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafqpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'templates/callosum2/L_Occipital.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mROCC_ni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafqpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'templates/callosum2/R_Occipital.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmidsag_ni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mafqpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'templates/callosum2/Callosum_midsag.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "LOCC_ni = nib.load(os.path.join(afqpath,'~/.AFQ/templates/callosum2/L_Occipital.nii.gz'))\n",
    "ROCC_ni = nib.load(os.path.join(afqpath,'~/.AFQ/templates/callosum2/R_Occipital.nii.gz'))\n",
    "midsag_ni = nib.load(os.path.join(afqpath,'~/.AFQ/templates/callosum2/Callosum_midsag.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
